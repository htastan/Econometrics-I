---
title: "Lab 05: Multiple Linear Regression - Estimation"
subtitle: "Econometrics I - YTU"
author: 
  name: "Prof. Dr. Hüseyin Taştan"
  affiliation: "Yıldız Technical University"
# date: "`r format(Sys.time(), '%d %B %Y')`"
date: 2020 Fall
output: 
  html_document:
    number_sections: true
    theme: readable
    highlight: haddock 
    # code_folding: show
    toc: yes
    toc_depth: 3
    toc_float: yes
    keep_md: true
---
<style type="text/css"> 
body{
  font-size: 12pt;
}
code.r{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'hold', fig.show = 'asis')
``` 

# Estimation of Multiple Linear Regression Model in R

The population regression model given by
  \[
        y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + u
  \]
can be estimated by the command
```{r, echo=TRUE, eval=FALSE}
lm(formula = y ~ x1 + x2 + ... + xk, options)
```
or just
```{r, echo=TRUE, eval=FALSE}
lm(y ~ x1 + x2 + ... + xk, options)
```
Note that the intercept is automatically added to the model. If you want to run 
a regression without a constant term (you should consider twice before doing that) you can use 
```{r, echo=TRUE, eval=FALSE}
lm(y ~ 0 + x1 + x2 + ... + xk, options)
```
The  ` lm() ` command accepts several options regarding the specification. See the help 
files. 

As we've already seen many times, if you want the results to be stored and 
displayed in a table format you can use 
```{r, echo=TRUE, eval=FALSE}
res <- lm(y ~ 0 + x1 + x2 + ... + xk, options)
summary(res)
```


## Example 1: College GPA
```{r, echo=TRUE}
library(wooldridge)
GPAres <- lm(colGPA ~ hsGPA+ACT, data=gpa1)
summary(GPAres)
```
SRF in equation form (standard errors are in parentheses): 
\[
 \widehat{colGPA} = \underset{(0.341)} {1.29} + \underset{(0.096)} {0.453}~ hsGPA + \underset{(0.011)} {0.0094}~ ACT
 \]
 $n=141$ students, $colGPA$: university grade point average (GPA, points out of 4), $hsGPA$: high school grade point average, $ACT$: achievement test score.

## Example 2: Wage Equation
```{r, echo=TRUE} 
summary( lm(log(wage) ~ educ+exper+tenure, data=wage1) )
``` 

## Example 3: Crime Model
```{r, echo=TRUE}  
# Model without avgsen:
summary( lm(narr86 ~ pcnv+ptime86+qemp86, data=crime1) )
``` 
Write down the SRF in equation form. Interpret the coefficient estimates. 
```{r, echo=TRUE}  
# Model with avgsen:
summary( lm(narr86 ~ pcnv+avgsen+ptime86+qemp86, data=crime1) )
``` 

# Imperfect Multicollinearity: Variance Inflation Factors (VIF)

In class we saw that the variance of OLS estimators can be written as 
\[
Var(\hat{\beta}_j)=\frac{\sigma^2}{SST_j(1-R_j^2)},~~j=1,2,\ldots,k
\]
where
\[
SST_j = \sum_{i=1}^n (x_{ij}-\bar{x}_j)^2
\]
is the total sample variation in $x_j$ and $R_j^2$ is the R-squared from regressing $x_j$ on all other independent variables (including an intercept term). The variance inflation factor (VIF) is defined as follows 
\[
VIF_j =\frac{1}{(1-R_j^2)},~~j=1,2,\ldots,k
\]

As an example consider the college GPA regression 
```{r, echo=TRUE}  
# Full estimation results including automatic SE :
res <- lm(colGPA ~ hsGPA+ACT, data=gpa1)
summary(res)
``` 
Compute the standard error of regression (SER) and the VIF: 
```{r, echo=TRUE, results='asis'} 
# Extract SER (instead of calculation via residuals)
( SER <- summary(res)$sigma )

# regressing hsGPA on ACT for calculation of R2 & VIF
( R2.hsGPA  <- summary( lm(hsGPA~ACT, data=gpa1) )$r.squared )
( VIF.hsGPA <- 1/(1-R2.hsGPA) )

# manual calculation of SE of hsGPA coefficient:
n <- nobs(res)
sdx <- sd(gpa1$hsGPA) * sqrt((n-1)/n)  # (Note: sd() uses the (n-1) version)
( SE.hsGPA <- 1/sqrt(n) * SER/sdx  * sqrt(VIF.hsGPA) )
``` 

$VIF_j$ measures the degree to which the variance is inflated due to the correlation of $x_j$ with other regressors in the model. 

Automatic Calculation of VIFs: 
```{r, echo=TRUE}  
# OLS regression:
lmres <- lm(log(wage) ~ educ+exper+tenure, data=wage1)

# Regression output:
summary(lmres)
```
```{r, echo=TRUE, results='asis'}
# Load package "car" (has to be installed):
library(car)
# Automatically calculate VIF :
vif(lmres)
``` 

**Rule of Thumb**: If the maximum $VIF_j$ is larger 10, then there may be (imperfect) collinearity in our regression. Because the largest VIF is less than 10 in our example we may be confident that there is no evidence of collinearity problem. 

# Omitted Variable Bias (OVB)
Let the true model be 
 $$
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
 $$

Assume that $x_2$ is omitted. Estimated regression is 
 $$
    \tilde{y }= \tilde{\beta}_0 + \tilde{\beta}_1 x_1
 $$
In class, we showed that the OLS estimator on $x_1$ is biased. In fact, the bias can theoretically be computed using 
 $$
 bias = E(\tilde{\beta}_1) - \beta_1 = \beta_2\tilde{\delta}_1
 $$
where 
 $$
 \tilde{x}_2 = \tilde{\delta}_0 + \tilde{\delta}_1 x_1
 $$
 
We can demonstrate the average bias by means of a Monte Carlo experiment. 
Let the true values of parameters be 
 $$
\beta_0 = 0, ~~\beta_1 = 1, ~~\beta_2=1, ~~\delta_0=0, ~~\delta_1=0.5
 $$
These values imply that 
 $$
  E(\tilde{\beta}_1) = \beta_1 + \beta_2\tilde{\delta}_1 = 1+ 1\times 0.5=1.5
 $$
 and
 $$
 bias =  \beta_2\tilde{\delta}_1 = 1\times 0.5 = 0.5
 $$

R code (also see `MLR_simulation.R` script)
```{r, echo=TRUE, results="hide", out.width = '80%', cache=TRUE}
# Set the random seed
# So that we will obtain the same results 
# Otherwise, simulation results will change 
set.seed(1234567)

# set sample size 
n <- 50 
# the number of simulations
MCreps <- 10000

# TRUE MODEL: y = beta0 + beta1 x1 + beta2 x2 + u
# ESTIMATED MODEL: y = b0tilde + b1tilde x1 + v 
# note that v = beta2 x2 + u
# Also: x2 = delta0 + delta1 x1 + epsilon 
#
# set true parameters: betas and sd of u
beta0 <- 0 
beta1 <- 1 
beta2 <- 1
su <- 2
delta0 <- 0 
delta1 <- 0.5 
sepsilon <- 1 

# initialize coefficient estimates to store results 
b0tilde <- numeric(MCreps)
b1tilde <- numeric(MCreps)
b0hat <- numeric(MCreps)
b1hat <- numeric(MCreps)

# Draw a sample of x1  
x1 <- 10*runif(n,0,1)

# repeat MCreps times:
for(i in 1:MCreps) {
  print(i)
  # Draw a sample of x1  
  x1 <- 10*runif(n,0,1)
  # Draw a sample of x2  
  x2 <- delta0 + delta1*x1 + rnorm(n,0,sepsilon)
  
  # Draw a sample of y:
  u <- rnorm(n,0,su)
  y <- beta0 + beta1*x1 + beta2*x2 + u
  # estimate parameters by OLS and store them in the vectors
  # case 1: OV
  bhat <- coefficients( lm(y ~ x1) )
  b0tilde[i] <- bhat["(Intercept)"]
  b1tilde[i] <- bhat["x1"]
  # case 2: No OV
  bhat2 <- coefficients( lm(y ~ x1 + x2) )
  b0hat[i] <- bhat2["(Intercept)"]
  b1hat[i] <- bhat2["x1"]
}

# smoothed histogram  
# create a data set consisting of betahats and betatildes
df <- data.frame(
  type=factor(rep(c("b1tilde", "b1hat"), each=MCreps)),
  bhat= c(b1tilde, b1hat) 
)

# compute group means 
library(plyr)
dfmean <- ddply(df, "type", summarise, bhat.mean=mean(bhat))

# draw smoothed histogram using ggplot2
library(ggplot2)
ggplot(df,aes(x=bhat, fill=type)) + geom_density(alpha=0.4) + 
  geom_vline(data=dfmean, aes(xintercept=bhat.mean,  colour=type),
             linetype="dashed", size=1) + 
  labs(x = "OLS estimates: NO OVB (left), OVB (right)")
```

Summary statistics: 
```{r, echo=TRUE, results='asis'}
# summary statistics
summary(b0tilde)
summary(b0hat)
```

```{r, echo=TRUE, results='asis'}
summary(b1tilde)
summary(b1hat)
``` 




<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>


